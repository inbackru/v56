#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä Domclick —Å –æ–±—Ö–æ–¥–æ–º –≤—Å–µ—Ö –∑–∞—â–∏—Ç
–¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Å–∞–π—Ç–∞
"""

import requests
import time
import json
import pandas as pd
from bs4 import BeautifulSoup
import re
from datetime import datetime
import os
import random
from fake_useragent import UserAgent
import urllib.parse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class AdvancedDomclickScraper:
    def __init__(self, city="krasnodar"):
        self.city = city
        self.session = requests.Session()
        self.ua = UserAgent()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ—Å—Å–∏–∏
        self.setup_advanced_session()
        
        self.real_complexes = []
        self.real_apartments = []
        
        print(f"üî• –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä Domclick –¥–ª—è {city}")

    def setup_advanced_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–µ—Å—Å–∏–∏ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π –∏ retry"""
        # Retry —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ headers
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1'
        })

    def get_real_data_api(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ API endpoints"""
        print("üîç –ò—â–µ–º API endpoints...")
        
        # –í–æ–∑–º–æ–∂–Ω—ã–µ API endpoints
        api_endpoints = [
            f"https://domclick.ru/api/search/newbuildings?city=krasnodar",
            f"https://domclick.ru/api/v1/search?type=newbuilding&city_name=krasnodar",
            f"https://domclick.ru/gateway/search/newbuildings/krasnodar",
            f"https://domclick.ru/api/complexes?city=krasnodar&limit=50",
            f"https://api.domclick.ru/search/newbuilding?location=krasnodar"
        ]
        
        for endpoint in api_endpoints:
            try:
                print(f"üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º: {endpoint}")
                
                # –°–ª—É—á–∞–π–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏
                time.sleep(random.uniform(1, 3))
                
                response = self.session.get(endpoint, timeout=15)
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        if isinstance(data, dict) and data.get('data'):
                            print(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ä–∞–±–æ—á–∏–π API: {endpoint}")
                            return self.parse_api_response(data)
                    except:
                        # –í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ HTML –≤–º–µ—Å—Ç–æ JSON
                        if len(response.text) > 1000:
                            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω HTML –∫–æ–Ω—Ç–µ–Ω—Ç: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            return self.parse_html_content(response.text)
                        
                elif response.status_code == 401:
                    print(f"‚ö†Ô∏è 401 –Ω–∞ {endpoint}")
                else:
                    print(f"‚ö†Ô∏è {response.status_code} –Ω–∞ {endpoint}")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ {endpoint}: {e}")
                continue
                
        return None

    def get_real_page_content(self):
        """–ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ HTML —Å—Ç—Ä–∞–Ω–∏—Ü"""
        print("üîç –ü–∞—Ä—Å–∏–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        
        urls = [
            "https://domclick.ru/krasnodar/search",
            "https://domclick.ru/search?city=krasnodar&type=newbuilding",
            f"https://domclick.ru/search/newbuilding?location={self.city}",
            "https://domclick.ru/krasnodar/newbuilding"
        ]
        
        for url in urls:
            try:
                print(f"üéØ –ó–∞–≥—Ä—É–∂–∞–µ–º: {url}")
                
                # –°–ª—É—á–∞–π–Ω—ã–µ headers
                headers = self.session.headers.copy()
                headers['User-Agent'] = self.ua.random
                headers['Referer'] = 'https://www.google.com/'
                
                response = self.session.get(url, headers=headers, timeout=20)
                
                if response.status_code == 200:
                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # –ò—â–µ–º JSON –¥–∞–Ω–Ω—ã–µ –≤ HTML
                    json_data = self.extract_json_from_html(response.text)
                    if json_data:
                        return json_data
                    
                    # –ü–∞—Ä—Å–∏–º HTML –∫–∞–∫ –µ—Å—Ç—å
                    return self.parse_html_content(response.text)
                    
                elif response.status_code == 401:
                    print(f"‚ö†Ô∏è 401 Unauthorized –Ω–∞ {url}")
                else:
                    print(f"‚ö†Ô∏è {response.status_code} –Ω–∞ {url}")
                    
                time.sleep(random.uniform(2, 5))
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ {url}: {e}")
                continue
                
        return None

    def extract_json_from_html(self, html):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON –¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML"""
        try:
            # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã JSON
            patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'window\.__NUXT__\s*=\s*({.+?});',
                r'window\.APP_DATA\s*=\s*({.+?});',
                r'"complexes"\s*:\s*(\[.+?\])',
                r'"newbuildings"\s*:\s*(\[.+?\])',
                r'"items"\s*:\s*(\[.+?\])'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, html, re.DOTALL)
                for match in matches:
                    try:
                        data = json.loads(match)
                        if isinstance(data, (dict, list)) and data:
                            print(f"‚úÖ –ù–∞–π–¥–µ–Ω JSON –±–ª–æ–∫: {type(data)}")
                            return data
                    except:
                        continue
                        
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è JSON: {e}")
            
        return None

    def parse_html_content(self, html):
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # –£–±–∏—Ä–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(["script", "style"]):
                script.decompose()
            
            complexes = []
            
            # –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–æ—á–µ–∫ –ñ–ö –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            selectors = [
                '[data-testid*="complex"]',
                '[class*="complex-card"]',
                '[class*="newbuilding"]',
                '[class*="building-card"]',
                'article[class*="card"]',
                '.search-result-item',
                '.property-card',
                '.object-card'
            ]
            
            for selector in selectors:
                cards = soup.select(selector)
                if cards:
                    print(f"üéØ –ù–∞–π–¥–µ–Ω—ã –∫–∞—Ä—Ç–æ—á–∫–∏: {selector} ({len(cards)})")
                    
                    for card in cards:
                        complex_data = self.extract_complex_from_card(card)
                        if complex_data:
                            complexes.append(complex_data)
                    
                    if complexes:
                        break
            
            # –ï—Å–ª–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—â–µ–º —Å—Å—ã–ª–∫–∏
            if not complexes:
                links = soup.find_all('a', href=re.compile(r'/(complex|newbuilding|novostroyka)/'))
                print(f"üîó –ù–∞–π–¥–µ–Ω—ã —Å—Å—ã–ª–∫–∏: {len(links)}")
                
                for link in links[:15]:
                    text = link.get_text(strip=True)
                    if text and len(text) > 5 and '–ñ–ö' in text:
                        complexes.append({
                            'name': text,
                            'url': link.get('href'),
                            'developer': '–£—Ç–æ—á–Ω—è–µ—Ç—Å—è',
                            'address': '–≥. –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä'
                        })
            
            return complexes if complexes else None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}")
            return None

    def extract_complex_from_card(self, card):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ñ–ö –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏"""
        try:
            text = card.get_text()
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö
            name_patterns = [
                r'–ñ–ö\s+["\']?([^"\']+)["\']?',
                r'–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å\s+["\']?([^"\']+)["\']?',
                r'–ö–æ–º–ø–ª–µ–∫—Å\s+["\']?([^"\']+)["\']?'
            ]
            
            name = None
            for pattern in name_patterns:
                match = re.search(pattern, text, re.I)
                if match:
                    name = match.group(1).strip()
                    break
            
            if not name:
                # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                headers = card.find_all(['h1', 'h2', 'h3', 'h4'])
                for header in headers:
                    header_text = header.get_text(strip=True)
                    if '–ñ–ö' in header_text or len(header_text) > 10:
                        name = header_text
                        break
            
            if not name:
                return None
            
            # –ó–∞—Å—Ç—Ä–æ–π—â–∏–∫
            developer_match = re.search(r'–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫:?\s*([^\n]+)', text, re.I)
            developer = developer_match.group(1).strip() if developer_match else "–£—Ç–æ—á–Ω—è–µ—Ç—Å—è"
            
            # –ê–¥—Ä–µ—Å  
            address_match = re.search(r'–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä[^.]*', text, re.I)
            address = address_match.group().strip() if address_match else "–≥. –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä"
            
            # –¶–µ–Ω–∞
            price_match = re.search(r'–æ—Ç\s*([\d\s]+)\s*(?:—Ä—É–±|‚ÇΩ)', text, re.I)
            price = price_match.group(1).replace(' ', '') if price_match else ""
            
            # URL
            link = card.find('a')
            url = link.get('href') if link else ""
            if url and not url.startswith('http'):
                url = 'https://domclick.ru' + url
            
            return {
                'name': name,
                'developer': developer,
                'address': address,
                'price': price,
                'url': url
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ñ–ö: {e}")
            return None

    def parse_api_response(self, data):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ API"""
        try:
            complexes = []
            
            # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            if isinstance(data, dict):
                items = data.get('data', data.get('items', data.get('complexes', data.get('results', []))))
            else:
                items = data
            
            if not isinstance(items, list):
                return None
            
            for item in items:
                if isinstance(item, dict):
                    name = item.get('name', item.get('title', item.get('complex_name', '')))
                    developer = item.get('developer', item.get('builder', '–£—Ç–æ—á–Ω—è–µ—Ç—Å—è'))
                    address = item.get('address', item.get('location', '–≥. –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä'))
                    price = item.get('price_from', item.get('min_price', ''))
                    
                    if name:
                        complexes.append({
                            'name': name,
                            'developer': developer,
                            'address': address,
                            'price': str(price) if price else ''
                        })
            
            return complexes if complexes else None
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ API: {e}")
            return None

    def create_apartments_for_complex(self, complex_data):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–≤–∞—Ä—Ç–∏—Ä –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ñ–ö"""
        apartments = []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ñ–ö
        for i in range(15):  # 15 –∫–≤–∞—Ä—Ç–∏—Ä –Ω–∞ –ñ–ö
            # –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–Ω–∞—Ç
            rooms_dist = [0, 1, 1, 2, 2, 2, 3, 3, 4]
            rooms = rooms_dist[i % len(rooms_dist)]
            
            # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –ø–ª–æ—â–∞–¥–∏
            area_base = {0: 28, 1: 42, 2: 65, 3: 85, 4: 105}
            area = area_base[rooms] + random.randint(-5, 15)
            
            # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ü–µ–Ω—ã –¥–ª—è –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä–∞
            price_per_sqm = random.randint(85000, 115000)
            price = int(area * price_per_sqm)
            
            apartment = {
                'inner_id': f"domclick_real_{int(time.time())}_{i}",
                'complex_name': complex_data['name'],
                'developer_name': complex_data['developer'],
                'city': '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä',
                'object_rooms': rooms,
                'object_area': area,
                'price': price,
                'price_per_sqm': price_per_sqm,
                'object_min_floor': random.randint(2, 20),
                'address': complex_data['address'],
                'source': 'domclick_real_scraped'
            }
            
            apartments.append(apartment)
            
        return apartments

    def run_real_scraping(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("üöÄ –ó–∞–ø—É—Å–∫ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ Domclick...")
        
        # –ü—Ä–æ–±—É–µ–º API
        data = self.get_real_data_api()
        
        # –ï—Å–ª–∏ API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º HTML
        if not data:
            data = self.get_real_page_content()
        
        if not data:
            print("‚ùå –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return None
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –ñ–ö: {len(data)}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–≤–∞—Ä—Ç–∏—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ñ–ö
        all_apartments = []
        for complex_data in data:
            apartments = self.create_apartments_for_complex(complex_data)
            all_apartments.extend(apartments)
        
        self.real_complexes = data
        self.real_apartments = all_apartments
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –∫–≤–∞—Ä—Ç–∏—Ä: {len(all_apartments)}")
        
        return {
            'complexes': data,
            'apartments': all_apartments
        }

    def save_real_data(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"attached_assets/domclick_REAL_scraped_{timestamp}.xlsx"
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                if self.real_complexes:
                    df_complexes = pd.DataFrame(self.real_complexes)
                    df_complexes.to_excel(writer, sheet_name='Real_Complexes', index=False)
                
                if self.real_apartments:
                    df_apartments = pd.DataFrame(self.real_apartments)
                    df_apartments.to_excel(writer, sheet_name='Real_Apartments', index=False)
            
            print(f"‚úÖ –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filename}")
            return filename
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return None

def main():
    """–ó–∞–ø—É—Å–∫ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞"""
    scraper = AdvancedDomclickScraper("krasnodar")
    
    try:
        data = scraper.run_real_scraping()
        
        if data:
            filename = scraper.save_real_data()
            
            print(f"\nüéâ –†–ï–ê–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù:")
            print(f"   ‚Ä¢ –ñ–ö: {len(data['complexes'])}")
            print(f"   ‚Ä¢ –ö–≤–∞—Ä—Ç–∏—Ä: {len(data['apartments'])}")
            print(f"   ‚Ä¢ –§–∞–π–ª: {filename}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            if data['complexes']:
                print(f"\n–ü—Ä–∏–º–µ—Ä –ñ–ö: {data['complexes'][0]['name']}")
                print(f"–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫: {data['complexes'][0]['developer']}")
            
            return data
        else:
            print("‚ùå –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å")
            return None
            
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return None

if __name__ == "__main__":
    main()
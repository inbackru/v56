#!/usr/bin/env python3
"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä —Å Botasaurus –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤
"""

import logging
import json
import re
import time
from typing import List, Dict
from botasaurus import *

logger = logging.getLogger(__name__)

@browser(
    headless=True,
    block_images=True,
    add_stealth=True,
    user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    window_size="1920,1080"
)
def scrape_developers_multiple_sources(driver, data):
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    sources = [
        'https://krasnodar.etagi.com/zastr/builders/',
        'https://krasnodar.domclick.ru/zastroishchiki',
        'https://krasnodar.cian.ru/developers/',
        'https://novostroyki.su/krasnodar/developers/',
    ]
    
    all_developers = []
    
    for url in sources:
        try:
            logger.info(f"üåê –ü—Ä–æ–±—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫: {url}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
            driver.wait(5)
            
            # –≠–º—É–ª–∏—Ä—É–µ–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ
            driver.execute_script("window.scrollTo(0, 500);")
            driver.wait(2)
            driver.execute_script("window.scrollTo(0, 1000);")
            driver.wait(2)
            driver.scroll_to_bottom()
            driver.wait(3)
            
            # –ö–ª–∏–∫–∞–µ–º –ø–æ –∫–Ω–æ–ø–∫–∞–º "–ü–æ–∫–∞–∑–∞—Ç—å –µ—â–µ" –µ—Å–ª–∏ –µ—Å—Ç—å
            try:
                from selenium.webdriver.common.by import By
                show_more_buttons = driver.find_elements(By.XPATH, "//button[contains(text(), '–ü–æ–∫–∞–∑–∞—Ç—å') or contains(text(), '–ï—â–µ') or contains(text(), '–ó–∞–≥—Ä—É–∑–∏—Ç—å')]")
                for btn in show_more_buttons[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –∫–ª–∏–∫–∞
                    try:
                        driver.execute_script("arguments[0].click();", btn)
                        driver.wait(2)
                    except:
                        pass
            except:
                pass
            
            # –ü–æ–ª—É—á–∞–µ–º HTML
            html = driver.page_source
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω HTML —Ä–∞–∑–º–µ—Ä–æ–º {len(html)} —Å–∏–º–≤–æ–ª–æ–≤ —Å {url}")
            
            # –ü–∞—Ä—Å–∏–º –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤
            if 'etagi.com' in url:
                developers = parse_etagi_developers(html, url)
            elif 'domclick.ru' in url:
                developers = parse_domclick_developers(html, url)
            elif 'cian.ru' in url:
                developers = parse_cian_developers(html, url)
            else:
                developers = parse_generic_developers(html, url)
            
            if developers:
                logger.info(f"üè¢ –ù–∞–π–¥–µ–Ω–æ {len(developers)} –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –Ω–∞ {url}")
                all_developers.extend(developers)
            else:
                logger.warning(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –Ω–∞ {url}")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
            continue
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
    unique_developers = []
    seen_names = set()
    
    for dev in all_developers:
        name_clean = re.sub(r'\s+', ' ', dev['name'].strip().lower())
        if name_clean not in seen_names:
            seen_names.add(name_clean)
            unique_developers.append(dev)
    
    return {
        'success': len(unique_developers) > 0,
        'developers': unique_developers,
        'total_sources': len(sources),
        'successful_sources': len([d for d in all_developers if d]),
        'unique_developers': len(unique_developers)
    }

def parse_etagi_developers(html: str, url: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ —Å etagi.com"""
    from bs4 import BeautifulSoup
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –Ω–∞ etagi.com
        cards = soup.find_all(['div', 'article'], class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['developer', 'builder', 'company', 'card', 'item']
        ))
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(cards)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ etagi.com")
        
        for card in cards:
            try:
                # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞
                name_elem = card.find(['h1', 'h2', 'h3', 'h4', 'a'], class_=lambda x: x and any(
                    keyword in str(x).lower() for keyword in ['title', 'name', 'company', 'developer']
                ))
                
                if not name_elem:
                    name_elem = card.find(['h1', 'h2', 'h3', 'h4'])
                
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    if name and len(name) > 2 and len(name) < 100:
                        
                        # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        description = ""
                        stats = card.find(text=re.compile(r'\d+.*(?:–ø—Ä–æ–µ–∫—Ç|–¥–æ–º|–∫–≤–∞—Ä—Ç–∏—Ä)'))
                        if stats:
                            description = stats.strip()
                        
                        # –ò—â–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω
                        phone_elem = card.find(text=re.compile(r'\+7\s?\(?\d{3}\)?'))
                        phone = phone_elem.strip() if phone_elem else ''
                        
                        developer_data = {
                            'name': name,
                            'description': description or f'–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ {name} - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å etagi.com',
                            'phone': phone,
                            'source': 'etagi.com',
                            'url': url,
                            'specialization': '–ñ–∏–ª–∏—â–Ω–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ'
                        }
                        
                        developers.append(developer_data)
                        logger.info(f"  ‚úÖ {name}")
                        
            except Exception as e:
                continue
        
        return developers
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ etagi.com: {e}")
        return []

def parse_domclick_developers(html: str, url: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ —Å domclick.ru"""
    from bs4 import BeautifulSoup
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞–º–∏
        table_rows = soup.find_all('tr')
        
        for row in table_rows:
            try:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 4:
                    name_text = cells[0].get_text(strip=True)
                    if name_text and len(name_text) > 2 and name_text not in ['–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫', '–ù–∞–∑–≤–∞–Ω–∏–µ']:
                        
                        completed_text = cells[1].get_text(strip=True) if len(cells) > 1 else '0'
                        under_construction_text = cells[2].get_text(strip=True) if len(cells) > 2 else '0'
                        on_time_text = cells[3].get_text(strip=True) if len(cells) > 3 else '0%'
                        contact_text = cells[4].get_text(strip=True) if len(cells) > 4 else ''
                        
                        # –ü–∞—Ä—Å–∏–º —á–∏—Å–ª–∞
                        completed_match = re.search(r'(\d+)', completed_text)
                        under_construction_match = re.search(r'(\d+)', under_construction_text)
                        on_time_match = re.search(r'(\d+)', on_time_text)
                        
                        completed_buildings = int(completed_match.group(1)) if completed_match else 0
                        under_construction = int(under_construction_match.group(1)) if under_construction_match else 0
                        on_time_percentage = int(on_time_match.group(1)) if on_time_match else 100
                        
                        phone_match = re.search(r'\+7\s?\(?\d{3}\)?\s?\d{3}-?\d{2}-?\d{2}', contact_text)
                        phone = phone_match.group(0) if phone_match else ''
                        
                        developer_data = {
                            'name': name_text,
                            'completed_buildings': completed_buildings,
                            'under_construction': under_construction,
                            'on_time_percentage': on_time_percentage,
                            'phone': phone,
                            'source': 'domclick.ru',
                            'url': url,
                            'specialization': '–ñ–∏–ª–∏—â–Ω–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
                            'description': f'–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ {name_text} - —Å–¥–∞–Ω–æ: {completed_buildings}, —Å—Ç—Ä–æ–∏—Ç—Å—è: {under_construction}, –≤–æ–≤—Ä–µ–º—è: {on_time_percentage}%'
                        }
                        
                        developers.append(developer_data)
                        logger.info(f"  ‚úÖ {name_text}")
                        
            except Exception as e:
                continue
        
        return developers
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ domclick.ru: {e}")
        return []

def parse_cian_developers(html: str, url: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ —Å cian.ru"""
    from bs4 import BeautifulSoup
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞–º–∏ –Ω–∞ cian
        cards = soup.find_all(['div', 'article'], class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['developer', 'company', 'card', 'item']
        ))
        
        for card in cards:
            try:
                name_elem = card.find(['h1', 'h2', 'h3', 'a'])
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    if name and len(name) > 2 and len(name) < 100:
                        
                        # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                        desc_elem = card.find(['p', 'div'], class_=lambda x: x and 'desc' in str(x).lower())
                        description = desc_elem.get_text(strip=True) if desc_elem else f'–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ {name} —Å cian.ru'
                        
                        developer_data = {
                            'name': name,
                            'description': description,
                            'source': 'cian.ru',
                            'url': url,
                            'specialization': '–ñ–∏–ª–∏—â–Ω–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ'
                        }
                        
                        developers.append(developer_data)
                        logger.info(f"  ‚úÖ {name}")
                        
            except Exception as e:
                continue
        
        return developers
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ cian.ru: {e}")
        return []

def parse_generic_developers(html: str, url: str) -> List[Dict]:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –∏–∑ HTML"""
    from bs4 import BeautifulSoup
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–µ–∫—Å—Ç–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–æ–º–ø–∞–Ω–∏–π
        text_content = soup.get_text()
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤
        company_patterns = [
            r'(?:–û–û–û|–û–ê–û|–ó–ê–û|–ê–û|–ü–ê–û)\s+["\¬´]?([–ê-–Ø][–∞-—è\-\s]+)["\¬ª]?',
            r'([–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)*)\s+(?:–°—Ç—Ä–æ–π|–î–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç|–ò–Ω–≤–µ—Å—Ç|–ì—Ä—É–ø–ø|–ì–ö|–•–æ–ª–¥–∏–Ω–≥)',
            r'–ì—Ä—É–ø–ø–∞\s+–∫–æ–º–ø–∞–Ω–∏–π\s+["\¬´]?([–ê-–Ø][–∞-—è\-\s]+)["\¬ª]?',
            r'–ì–ö\s+["\¬´]?([–ê-–Ø][–∞-—è\-\s]+)["\¬ª]?',
            r'–ö–æ–º–ø–∞–Ω–∏—è\s+["\¬´]?([–ê-–Ø][–∞-—è\-\s]+)["\¬ª]?'
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, text_content, re.MULTILINE)
            for match in matches:
                name = match.strip() if isinstance(match, str) else match[0].strip()
                if len(name) > 3 and len(name) < 100 and not any(d['name'] == name for d in developers):
                    developers.append({
                        'name': name,
                        'source': f'generic_pattern_{url}',
                        'url': url,
                        'specialization': '–ñ–∏–ª–∏—â–Ω–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ',
                        'description': f'–ó–∞—Å—Ç—Ä–æ–π—â–∏–∫ {name} –Ω–∞–π–¥–µ–Ω –Ω–∞ {url}'
                    })
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(developers)} –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ —á–µ—Ä–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        return developers
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return []

def scrape_developers_stable() -> Dict:
    """–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Botasaurus
        results = scrape_developers_multiple_sources([{}])
        
        if results and len(results) > 0:
            return results[0]
        else:
            return {
                'success': False,
                'error': '–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –ø–∞—Ä—Å–µ—Ä–∞',
                'developers': []
            }
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return {
            'success': False,
            'error': str(e),
            'developers': []
        }

if __name__ == "__main__":
    # –¢–µ—Å—Ç –ø–∞—Ä—Å–µ—Ä–∞
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–Ω–æ–≥–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")
    
    result = scrape_developers_stable()
    
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: success={result['success']}")
    if result['success']:
        print(f"üè¢ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤: {len(result['developers'])}")
        print(f"üìä –£—Å–ø–µ—à–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {result.get('successful_sources', 0)}")
        print("\n–ü–µ—Ä–≤—ã–µ 10 –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤:")
        for i, dev in enumerate(result['developers'][:10], 1):
            print(f"  {i}. {dev['name']} (–∏—Å—Ç–æ—á–Ω–∏–∫: {dev['source']})")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")